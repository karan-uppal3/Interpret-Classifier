{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alexnet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0cC6lsOkBzoW",
        "Y2b6_Uanu_UA",
        "rx46Oe5UIL2U",
        "M2nqnK6BHFMi",
        "E_8V2UWxLHCy",
        "dccCQfKgPniP",
        "qPO08dG97U8o"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hhhLFd4CscP"
      },
      "source": [
        "n_classes = 3\n",
        "\n",
        "# Location of data\n",
        "datadir = '/content/drive/MyDrive/Interpretable Classifier Data/Vanilla Classifier Data/'\n",
        "traindir = datadir + 'train/'\n",
        "validdir = datadir + 'valid/'\n",
        "testdir = datadir + 'test/'\n",
        "\n",
        "# Change to fit hardware\n",
        "batch_size = 10\n",
        "\n",
        "train_on_gpu = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cC6lsOkBzoW"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XozmFNl4B69f"
      },
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import glob\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2b6_Uanu_UA"
      },
      "source": [
        "# Loading data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXegnlGP1QCo"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform_img = transforms.Compose([\n",
        "  transforms.Resize(size=256),\n",
        "  transforms.CenterCrop(size=224),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPMQggQbCTrf"
      },
      "source": [
        "# Datasets from each folder\n",
        "data = {\n",
        "    'train':\n",
        "    datasets.ImageFolder(root=traindir, transform=transform_img),\n",
        "    'val':\n",
        "    datasets.ImageFolder(root=validdir, transform=transform_img),\n",
        "    'test':\n",
        "    datasets.ImageFolder(root=testdir, transform=transform_img)\n",
        "}\n",
        "\n",
        "# Dataloader iterators\n",
        "dataloaders = {\n",
        "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True),\n",
        "    'val': DataLoader(data['val'], batch_size=1, shuffle=True),\n",
        "    'test': DataLoader(data['test'], batch_size=1, shuffle=True)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx46Oe5UIL2U"
      },
      "source": [
        "# Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRPMTPuSIwum"
      },
      "source": [
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-iFua7ogXtv"
      },
      "source": [
        "def evaluate(model,val_loader,epoch,criterion):\n",
        "\n",
        "  acc = 0\n",
        "  epoch_loss = 0.0\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  for img,label in val_loader:\n",
        "\n",
        "    model.eval()\n",
        "  \n",
        "    xb = img.to('cuda')\n",
        "    label = label.to('cuda').long()\n",
        "\n",
        "    yb = model(xb)\n",
        "\n",
        "    loss = criterion(yb, label)\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    pred = F.softmax(yb, dim=1)\n",
        "\n",
        "    _, preds  = torch.max(yb, dim=1)\n",
        "\n",
        "    preds = preds.cpu()\n",
        "    label = label.cpu()\n",
        "\n",
        "    temp = (np.array( preds == label )).sum()\n",
        "    acc += temp\n",
        "\n",
        "  print(\"Accuracy = \",acc*100/len(val_loader))\n",
        "  print(\"------------\")\n",
        "\n",
        "  return {\"Accuracy\":acc*100/len(val_loader),\"Loss\":epoch_loss/len(val_loader)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6GO4V4OgS7Q"
      },
      "source": [
        "%mkdir checkpoint best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTxuEMbfNk6S"
      },
      "source": [
        "import shutil\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS8Et5k8NpLZ"
      },
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into       \n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS3W078LRKbf"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader,criterion,opt_func=torch.optim.Adam,checkpoint_path=\"/content/checkpoint/current_checkpoint.pt\", best_model_path=\"/content/best_model/best_model.pt\"):\n",
        "\n",
        "    epoch_data = {}\n",
        "    optim = opt_func(model.parameters(),lr=lr,weight_decay=0.0005)\n",
        "\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for step, batch_data in enumerate(train_loader):\n",
        "\n",
        "            # Get the inputs and labels\n",
        "            inputs = batch_data[0].to('cuda')\n",
        "            labels = batch_data[1].to('cuda').long()\n",
        "\n",
        "            # Forward propagation\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Loss computation\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backpropagation\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            # Keep track of loss for current epoch\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_data[epoch+1] = evaluate(model,val_loader,epoch+1,criterion)\n",
        "        val_loss = epoch_data[epoch+1][\"Loss\"]\n",
        "\n",
        "        print(\"Epoch number:\",epoch+1,\" - Training Loss = \" , epoch_loss / len(train_loader) , \" Valiadtion Loss = \", val_loss )\n",
        "\n",
        "        wandb.log( { \"Training Loss\" : epoch_loss / len(train_loader) , \n",
        "                     \"Validation Loss\" : val_loss } )\n",
        "\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': val_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optim.state_dict(),\n",
        "        }\n",
        "\n",
        "        save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "\n",
        "        if val_loss <= valid_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,val_loss))\n",
        "            # save checkpoint as best model\n",
        "            save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "            valid_loss_min = val_loss\n",
        "\n",
        "        print(\"------------\")\n",
        "\n",
        "    return epoch_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2nqnK6BHFMi"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaGu3pHfgrTC"
      },
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=False)\n",
        "model.classifier[6] = nn.Linear(4096,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJxYWa4mg0YP"
      },
      "source": [
        "model = model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8V2UWxLHCy"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Koeen4WZukw1"
      },
      "source": [
        "num_epochs = 100\n",
        "opt_func = torch.optim.Adam\n",
        "lr = 0.0001\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8DGKs0vzzWp"
      },
      "source": [
        "## Wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqrkT4pky-ec"
      },
      "source": [
        "!pip3 install wandb\n",
        "import wandb\n",
        "wandb.login()\n",
        "wandb.init(project=\"AlexNet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quZunoFILVlB"
      },
      "source": [
        "epoch_data = fit(num_epochs, lr, model, dataloaders['train'], dataloaders['val'] , criterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dccCQfKgPniP"
      },
      "source": [
        "# Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz0W2VwYh_LT"
      },
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=False)\n",
        "model.classifier[6] = nn.Linear(4096,3)\n",
        "model = model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rweJVsbxQReK"
      },
      "source": [
        "# define optimzer\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# define checkpoint saved path\n",
        "ckp_path = \"/content/drive/MyDrive/Interpretable Classifier Data/Vanilla models/alexnet-final.pt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvIoLs_8Qj_a"
      },
      "source": [
        "model, optimizer, start_epoch, valid_loss_min = load_ckp(ckp_path, model, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6TFlL14QmCA"
      },
      "source": [
        "print(\"start_epoch = \", start_epoch)\n",
        "print(\"valid_loss_min = \", valid_loss_min)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPO08dG97U8o"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqh2ICyg6zz9"
      },
      "source": [
        "y_pred = []\n",
        "y_true = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvSy78UO88HD"
      },
      "source": [
        "acc = 0\n",
        "epoch_loss = 0.0\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for img, label in dataloaders['test']:\n",
        "\n",
        "  model.eval()\n",
        "  \n",
        "  xb = img.to('cuda')\n",
        "  label = label.to('cuda').long()\n",
        "\n",
        "  yb = model(xb)\n",
        "\n",
        "  pred = F.softmax(yb, dim=1)\n",
        "  _, preds  = torch.max(yb, dim=1)\n",
        "\n",
        "  preds = preds.cpu()\n",
        "  label = label.cpu()\n",
        "\n",
        "  y_pred.append(int(preds))\n",
        "  y_true.append(int(label))\n",
        "\n",
        "  temp = (np.array( preds == label )).sum()\n",
        "  acc += temp\n",
        "\n",
        "print(\"Accuracy = \",acc*100/len(dataloaders['test']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ItOdlXYUFJ3"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_true, y_pred)*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SuVJgqa6u-W"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}