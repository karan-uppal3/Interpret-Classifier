{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg16.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eAA-XjJs26_6",
        "74CULkZB31LB",
        "ZmaJ-QLn4nFg",
        "hIec-I-x5KWx",
        "TDUnJ3vYKVnA",
        "hlMmIf7RKVnB",
        "FLqv6g1C_Umv",
        "NTMnMCCm5lnZ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "emVynHKN3Bws"
      },
      "source": [
        "n_classes = 3\r\n",
        "\r\n",
        "# Location of data\r\n",
        "datadir = '/content/drive/MyDrive/Interpretable Classifier Data/Vanilla Classifier Data/'\r\n",
        "traindir = datadir + 'train/'\r\n",
        "validdir = datadir + 'valid/'\r\n",
        "testdir = datadir + 'test/'\r\n",
        "\r\n",
        "save_file_name = 'vgg16-transfer-4.pt'\r\n",
        "checkpoint_path = 'vgg16-transfer-4.pth'\r\n",
        "\r\n",
        "# Change to fit hardware\r\n",
        "batch_size = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAA-XjJs26_6"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcJiACPf23XE"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "import seaborn as sns\r\n",
        "# PyTorch\r\n",
        "from torchvision import transforms, datasets, models\r\n",
        "import torch\r\n",
        "from torch import optim, cuda\r\n",
        "from torch.utils.data import DataLoader, sampler\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\r\n",
        "\r\n",
        "# Data science tools\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "from PIL import Image\r\n",
        "import cv2\r\n",
        "import glob\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Image manipulations\r\n",
        "from PIL import Image\r\n",
        "# Useful for examining network\r\n",
        "from torchsummary import summary\r\n",
        "# Timing utility\r\n",
        "from timeit import default_timer as timer\r\n",
        "\r\n",
        "# Visualizations\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "plt.rcParams['font.size'] = 14\r\n",
        "\r\n",
        "# Printing out all outputs\r\n",
        "InteractiveShell.ast_node_interactivity = 'all'\r\n",
        "\r\n",
        "# Whether to train on a gpu\r\n",
        "train_on_gpu = cuda.is_available()\r\n",
        "print(f'Train on gpu: {train_on_gpu}')\r\n",
        "\r\n",
        "# Number of gpus\r\n",
        "if train_on_gpu:\r\n",
        "    gpu_count = cuda.device_count()\r\n",
        "    print(f'{gpu_count} gpus detected.')\r\n",
        "    if gpu_count > 1:\r\n",
        "        multi_gpu = True\r\n",
        "    else:\r\n",
        "        multi_gpu = False\r\n",
        "else:\r\n",
        "    multi_gpu = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74CULkZB31LB"
      },
      "source": [
        "# Loading  Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bdrQKhB3-XQ"
      },
      "source": [
        "# Image transformations\r\n",
        "image_transforms = {\r\n",
        "    \r\n",
        "    'train':\r\n",
        "    transforms.Compose([\r\n",
        "        transforms.Resize(size=256),\r\n",
        "        transforms.CenterCrop(size=224),  # Image net standards\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\r\n",
        "                             [0.229, 0.224, 0.225])  # Imagenet standards\r\n",
        "    ]),\r\n",
        "    \r\n",
        "    'val':\r\n",
        "    transforms.Compose([\r\n",
        "        transforms.Resize(size=256),\r\n",
        "        transforms.CenterCrop(size=224),\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\r\n",
        "    ]),\r\n",
        "    \r\n",
        "    'test':\r\n",
        "    transforms.Compose([\r\n",
        "        transforms.Resize(size=256),\r\n",
        "        transforms.CenterCrop(size=224),\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\r\n",
        "    ]),\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvganc7r4Huh"
      },
      "source": [
        "# Datasets from each folder\r\n",
        "data = {\r\n",
        "    'train':\r\n",
        "    datasets.ImageFolder(root=traindir, transform=image_transforms['train']),\r\n",
        "    'val':\r\n",
        "    datasets.ImageFolder(root=validdir, transform=image_transforms['val']),\r\n",
        "    'test':\r\n",
        "    datasets.ImageFolder(root=testdir, transform=image_transforms['test'])\r\n",
        "}\r\n",
        "\r\n",
        "# Dataloader iterators\r\n",
        "dataloaders = {\r\n",
        "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True),\r\n",
        "    'val': DataLoader(data['val'], batch_size=batch_size, shuffle=True),\r\n",
        "    'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True)\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmaJ-QLn4nFg"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFozgEZG4Qq3"
      },
      "source": [
        "model = models.vgg16(pretrained=True)\r\n",
        "\r\n",
        "# Freeze early layers\r\n",
        "for param in model.parameters():\r\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGWd1dHt4rKt"
      },
      "source": [
        "n_inputs = model.classifier[6].in_features\r\n",
        "\r\n",
        "# Add on classifier\r\n",
        "model.classifier[6] = nn.Sequential(\r\n",
        "    nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.4),\r\n",
        "    nn.Linear(256, 3), nn.LogSoftmax(dim=1))\r\n",
        "\r\n",
        "model.classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0hGRyiT4suH"
      },
      "source": [
        "if train_on_gpu:\r\n",
        "    model = model.to('cuda')\r\n",
        "\r\n",
        "if multi_gpu:\r\n",
        "    model = nn.DataParallel(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njf41xYK4vwz"
      },
      "source": [
        "model.class_to_idx = data['train'].class_to_idx\r\n",
        "model.idx_to_class = {\r\n",
        "    idx: class_\r\n",
        "    for class_, idx in model.class_to_idx.items()\r\n",
        "}\r\n",
        "\r\n",
        "list(model.idx_to_class.items())[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIec-I-x5KWx"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvNq5w8T5J94"
      },
      "source": [
        "criterion = nn.NLLLoss()\r\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYtMZNfjKVm_"
      },
      "source": [
        "def train(model,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          train_loader,\n",
        "          valid_loader,\n",
        "          save_file_name,\n",
        "          max_epochs_stop=3,\n",
        "          n_epochs=20,\n",
        "          print_every=2):\n",
        "    \"\"\"Train a PyTorch Model\n",
        "\n",
        "    Params\n",
        "    --------\n",
        "        model (PyTorch model): cnn to train\n",
        "        criterion (PyTorch loss): objective to minimize\n",
        "        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n",
        "        train_loader (PyTorch dataloader): training dataloader to iterate through\n",
        "        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n",
        "        save_file_name (str ending in '.pt'): file path to save the model state dict\n",
        "        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n",
        "        n_epochs (int): maximum number of training epochs\n",
        "        print_every (int): frequency of epochs to print training stats\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "        model (PyTorch model): trained cnn with best weights\n",
        "        history (DataFrame): history of train and validation loss and accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    # Early stopping intialization\n",
        "    epochs_no_improve = 0\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    valid_max_acc = 0\n",
        "    history = []\n",
        "\n",
        "    # Number of epochs already trained (if using loaded in model weights)\n",
        "    try:\n",
        "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
        "    except:\n",
        "        model.epochs = 0\n",
        "        print(f'Starting Training from Scratch.\\n')\n",
        "\n",
        "    overall_start = timer()\n",
        "\n",
        "    # Main loop\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # keep track of training and validation loss each epoch\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        train_acc = 0\n",
        "        valid_acc = 0\n",
        "\n",
        "        # Set to training\n",
        "        model.train()\n",
        "        start = timer()\n",
        "\n",
        "        # Training loop\n",
        "        for ii, (data, target) in enumerate(train_loader):\n",
        "            # Tensors to gpu\n",
        "            if train_on_gpu:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Predicted outputs are log probabilities\n",
        "            output = model(data)\n",
        "\n",
        "            # Loss and backpropagation of gradients\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track train loss by multiplying average loss by number of examples in batch\n",
        "            train_loss += loss.item() * data.size(0)\n",
        "\n",
        "            # Calculate accuracy by finding max log probability\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "            # Need to convert correct tensor from int to float to average\n",
        "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
        "            # Multiply average accuracy times the number of examples in batch\n",
        "            train_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "            # Track training progress\n",
        "            print(\n",
        "                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
        "                end='\\r')\n",
        "\n",
        "        # After training loops ends, start validation\n",
        "        else:\n",
        "            model.epochs += 1\n",
        "\n",
        "            # Don't need to keep track of gradients\n",
        "            with torch.no_grad():\n",
        "                # Set to evaluation mode\n",
        "                model.eval()\n",
        "\n",
        "                # Validation loop\n",
        "                for data, target in valid_loader:\n",
        "                    # Tensors to gpu\n",
        "                    if train_on_gpu:\n",
        "                        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "                    # Forward pass\n",
        "                    output = model(data)\n",
        "\n",
        "                    # Validation loss\n",
        "                    loss = criterion(output, target)\n",
        "                    # Multiply average loss times the number of examples in batch\n",
        "                    valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "                    # Calculate validation accuracy\n",
        "                    _, pred = torch.max(output, dim=1)\n",
        "                    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "                    accuracy = torch.mean(\n",
        "                        correct_tensor.type(torch.FloatTensor))\n",
        "                    # Multiply average accuracy times the number of examples\n",
        "                    valid_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "                # Calculate average losses\n",
        "                train_loss = train_loss / len(train_loader.dataset)\n",
        "                valid_loss = valid_loss / len(valid_loader.dataset)\n",
        "\n",
        "                # Calculate average accuracy\n",
        "                train_acc = train_acc / len(train_loader.dataset)\n",
        "                valid_acc = valid_acc / len(valid_loader.dataset)\n",
        "\n",
        "                history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
        "\n",
        "                # Print training and validation results\n",
        "                if (epoch + 1) % print_every == 0:\n",
        "                    print(\n",
        "                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
        "                    )\n",
        "                    print(\n",
        "                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n",
        "                    )\n",
        "\n",
        "                # Save the model if validation loss decreases\n",
        "                if valid_loss < valid_loss_min:\n",
        "                    # Save model\n",
        "                    torch.save(model.state_dict(), save_file_name)\n",
        "                    # Track improvement\n",
        "                    epochs_no_improve = 0\n",
        "                    valid_loss_min = valid_loss\n",
        "                    valid_best_acc = valid_acc\n",
        "                    best_epoch = epoch\n",
        "\n",
        "                # Otherwise increment count of epochs with no improvement\n",
        "                else:\n",
        "                    epochs_no_improve += 1\n",
        "                    # Trigger early stopping\n",
        "                    if epochs_no_improve >= max_epochs_stop:\n",
        "                        print(\n",
        "                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "                        )\n",
        "                        total_time = timer() - overall_start\n",
        "                        print(\n",
        "                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
        "                        )\n",
        "\n",
        "                        # Load the best state dict\n",
        "                        model.load_state_dict(torch.load(save_file_name))\n",
        "                        # Attach the optimizer\n",
        "                        model.optimizer = optimizer\n",
        "\n",
        "                        # Format history\n",
        "                        history = pd.DataFrame(\n",
        "                            history,\n",
        "                            columns=[\n",
        "                                'train_loss', 'valid_loss', 'train_acc',\n",
        "                                'valid_acc'\n",
        "                            ])\n",
        "                        return model, history\n",
        "\n",
        "    # Attach the optimizer\n",
        "    model.optimizer = optimizer\n",
        "    # Record overall time and print out stats\n",
        "    total_time = timer() - overall_start\n",
        "    print(\n",
        "        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "    )\n",
        "    print(\n",
        "        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.'\n",
        "    )\n",
        "    # Format history\n",
        "    history = pd.DataFrame(\n",
        "        history,\n",
        "        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
        "    return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSOFgovmKVm_"
      },
      "source": [
        "model, history = train(\n",
        "    model,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    dataloaders['train'],\n",
        "    dataloaders['val'],\n",
        "    save_file_name=save_file_name,\n",
        "    max_epochs_stop=5,\n",
        "    n_epochs=30,\n",
        "    print_every=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDUnJ3vYKVnA"
      },
      "source": [
        "# Training Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qoc4Y14KVnA"
      },
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "for c in ['train_loss', 'valid_loss']:\n",
        "    plt.plot(\n",
        "        history[c], label=c)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Negative Log Likelihood')\n",
        "plt.title('Training and Validation Losses')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TaD1V30KVnA"
      },
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "for c in ['train_acc', 'valid_acc']:\n",
        "    plt.plot(\n",
        "        100 * history[c], label=c)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.title('Training and Validation Accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlMmIf7RKVnB"
      },
      "source": [
        "# Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eFRwcZ7KVnB"
      },
      "source": [
        "def save_checkpoint(model, path):\n",
        "    \"\"\"Save a PyTorch model checkpoint\n",
        "\n",
        "    Params\n",
        "    --------\n",
        "        model (PyTorch model): model to save\n",
        "        path (str): location to save model. Must start with `model_name-` and end in '.pth'\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "        None, save the `model` to `path`\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Basic details\n",
        "    checkpoint = {\n",
        "        'class_to_idx': model.class_to_idx,\n",
        "        'idx_to_class': model.idx_to_class,\n",
        "        'epochs': model.epochs,\n",
        "    }\n",
        "\n",
        "    # Extract the final classifier and the state dictionary\n",
        "    if multi_gpu:\n",
        "        checkpoint['classifier'] = model.module.classifier\n",
        "        checkpoint['state_dict'] = model.module.state_dict()\n",
        "    else:\n",
        "        checkpoint['classifier'] = model.classifier\n",
        "        checkpoint['state_dict'] = model.state_dict()\n",
        "\n",
        "    # Add the optimizer\n",
        "    checkpoint['optimizer'] = model.optimizer\n",
        "    checkpoint['optimizer_state_dict'] = model.optimizer.state_dict()\n",
        "\n",
        "    # Save the data to the path\n",
        "    torch.save(checkpoint, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVUC4TbNKVnB"
      },
      "source": [
        "save_checkpoint(model, path=checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLqv6g1C_Umv"
      },
      "source": [
        "# Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PklYQP47KVnC"
      },
      "source": [
        "def load_checkpoint(path):\n",
        "    \"\"\"Load a PyTorch model checkpoint\n",
        "\n",
        "    Params\n",
        "    --------\n",
        "        path (str): saved model checkpoint. Must start with `model_name-` and end in '.pth'\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "        None, save the `model` to `path`\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Load in checkpoint\n",
        "    checkpoint = torch.load(path)\n",
        "\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    # Make sure to set parameters as not trainable\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    model.classifier = checkpoint['classifier']\n",
        "\n",
        "    # Load in the state dict\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'{total_params:,} total parameters.')\n",
        "    total_trainable_params = sum(\n",
        "        p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'{total_trainable_params:,} total gradient parameters.')\n",
        "\n",
        "    # Move to gpu\n",
        "    if multi_gpu:\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    if train_on_gpu:\n",
        "        model = model.to('cuda')\n",
        "\n",
        "    # Model basics\n",
        "    model.class_to_idx = checkpoint['class_to_idx']\n",
        "    model.idx_to_class = checkpoint['idx_to_class']\n",
        "    model.epochs = checkpoint['epochs']\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = checkpoint['optimizer']\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    return model, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxzhawL5KVnC"
      },
      "source": [
        "model, optimizer = load_checkpoint(path='/content/drive/MyDrive/Interpretable Classifier Data/Vanilla models/vgg16-final.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTMnMCCm5lnZ"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "840OjMlPKVnD"
      },
      "source": [
        "def process_image(image_path):\n",
        "    \"\"\"Process an image path into a PyTorch tensor\"\"\"\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    # Resize\n",
        "    img = image.resize((256, 256))\n",
        "\n",
        "    # Center crop\n",
        "    width = 256\n",
        "    height = 256\n",
        "    new_width = 224\n",
        "    new_height = 224\n",
        "\n",
        "    left = (width - new_width) / 2\n",
        "    top = (height - new_height) / 2\n",
        "    right = (width + new_width) / 2\n",
        "    bottom = (height + new_height) / 2\n",
        "    img = img.crop((left, top, right, bottom))\n",
        "\n",
        "    # Convert to numpy, transpose color dimension and normalize\n",
        "    img = np.array(img).transpose((2, 0, 1)) / 256\n",
        "\n",
        "    # Standardization\n",
        "    means = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
        "    stds = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
        "\n",
        "    img = img - means\n",
        "    img = img / stds\n",
        "\n",
        "    img_tensor = torch.Tensor(img)\n",
        "\n",
        "    return img_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYuZ46r8KVnH"
      },
      "source": [
        "def accuracy(output, target, topk=(1,3)):\n",
        "    \"\"\"Compute the topk accuracy(s)\"\"\"\n",
        "    if train_on_gpu:\n",
        "        output = output.to('cuda')\n",
        "        target = target.to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        # Find the predicted classes and transpose\n",
        "        _, pred = output.topk(k=maxk, dim=1, largest=True, sorted=True)\n",
        "        pred = pred.t()\n",
        "\n",
        "        # Determine predictions equal to the targets\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "\n",
        "        # For each k, find the percentage of correct\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size).item())\n",
        "        return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqR-6CX7KVnI"
      },
      "source": [
        "def evaluate(model, test_loader, criterion, topk=(1, 3)):\n",
        "    \"\"\"Measure the performance of a trained PyTorch model\n",
        "\n",
        "    Params\n",
        "    --------\n",
        "        model (PyTorch model): trained cnn for inference\n",
        "        test_loader (PyTorch DataLoader): test dataloader\n",
        "        topk (tuple of ints): accuracy to measure\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "        results (DataFrame): results for each category\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    classes = []\n",
        "    losses = []\n",
        "    # Hold accuracy results\n",
        "    acc_results = np.zeros((len(test_loader.dataset), len(topk)))\n",
        "    i = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Testing loop\n",
        "        for data, targets in test_loader:\n",
        "\n",
        "            # Tensors to gpu\n",
        "            if train_on_gpu:\n",
        "                data, targets = data.to('cuda'), targets.to('cuda')\n",
        "\n",
        "            # Raw model output\n",
        "            out = model(data)\n",
        "            # Iterate through each example\n",
        "            for pred, true in zip(out, targets):\n",
        "                # Find topk accuracy\n",
        "                acc_results[i, :] = accuracy(\n",
        "                    pred.unsqueeze(0), true.unsqueeze(0), topk)\n",
        "                classes.append(model.idx_to_class[true.item()])\n",
        "                # Calculate the loss\n",
        "                loss = criterion(pred.view(1, n_classes), true.view(1))\n",
        "                losses.append(loss.item())\n",
        "                i += 1\n",
        "\n",
        "    # Send results to a dataframe and calculate average across classes\n",
        "    results = pd.DataFrame(acc_results, columns=[f'top{i}' for i in topk])\n",
        "    results['class'] = classes\n",
        "    results['loss'] = losses\n",
        "    results = results.groupby(classes).mean()\n",
        "\n",
        "    return results.reset_index().rename(columns={'index': 'class'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0QyU5u1KVnI"
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "# Evaluate the model on the testing data\n",
        "results = evaluate(model, dataloaders['test'], criterion)\n",
        "results.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}